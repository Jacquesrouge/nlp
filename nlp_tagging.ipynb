{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r http://webia.lip6.fr/~bpiwowar/requirements-amal.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading datasets...\n",
      "INFO:root:Vocabulary size: 42930\n"
     ]
    }
   ],
   "source": [
    "# %load C:\\Users\\jacqu\\Scolarite\\M2A\\AMAL\\student_tp6\\src\\tp6-tagging.py\n",
    "import itertools\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from datamaestro import prepare_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from typing import List\n",
    "import time\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ds = prepare_dataset('org.universaldependencies.french.gsd')\n",
    "\n",
    "\n",
    "# Format de sortie décrit dans\n",
    "# https://pypi.org/project/conllu/\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Permet de gérer un vocabulaire.\n",
    "\n",
    "    En test, il est possible qu'un mot ne soit pas dans le\n",
    "    vocabulaire : dans ce cas le token \"__OOV__\" est utilisé.\n",
    "    Attention : il faut tenir compte de cela lors de l'apprentissage !\n",
    "\n",
    "    Utilisation:\n",
    "\n",
    "    - en train, utiliser v.get(\"blah\", adding=True) pour que le mot soit ajouté\n",
    "      automatiquement\n",
    "    - en test, utiliser v[\"blah\"] pour récupérer l'ID du mot (ou l'ID de OOV)\n",
    "    \"\"\"\n",
    "    OOVID = 1\n",
    "    PAD = 0\n",
    "\n",
    "    def __init__(self, oov: bool):\n",
    "        self.oov =  oov\n",
    "        self.id2word = [ \"PAD\"]\n",
    "        self.word2id = { \"PAD\" : Vocabulary.PAD}\n",
    "        if oov:\n",
    "            self.word2id[\"__OOV__\"] = Vocabulary.OOVID\n",
    "            self.id2word.append(\"__OOV__\")\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        if self.oov:\n",
    "            return self.word2id.get(word, Vocabulary.OOVID)\n",
    "        return self.word2id[word]\n",
    "\n",
    "    def get(self, word: str, adding=True):\n",
    "        try:\n",
    "            return self.word2id[word]\n",
    "        except KeyError:\n",
    "            if adding:\n",
    "                wordid = len(self.id2word)\n",
    "                self.word2id[word] = wordid\n",
    "                self.id2word.append(word)\n",
    "                return wordid\n",
    "            if self.oov:\n",
    "                return Vocabulary.OOVID\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id2word)\n",
    "\n",
    "    def getword(self,idx: int):\n",
    "        if idx < len(self):\n",
    "            return self.id2word[idx]\n",
    "        return None\n",
    "\n",
    "    def getwords(self,idx: List[int]):\n",
    "        return [self.getword(i) for i in idx]\n",
    "\n",
    "\n",
    "\n",
    "class TaggingDataset():\n",
    "    def __init__(self, data, words: Vocabulary, tags: Vocabulary, adding=True):\n",
    "        self.sentences = []\n",
    "\n",
    "        for s in data:\n",
    "            self.sentences.append(([words.get(token[\"form\"], adding) for token in s], [tags.get(token[\"upostag\"], adding) for token in s]))\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    def __getitem__(self, ix):\n",
    "        return self.sentences[ix]\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    \"\"\"Collate using pad_sequence\"\"\"\n",
    "    return tuple(pad_sequence([torch.LongTensor(b[j]) for b in batch]) for j in range(2))\n",
    "    \n",
    "\n",
    "\n",
    "logging.info(\"Loading datasets...\")\n",
    "words = Vocabulary(True)\n",
    "tags = Vocabulary(False)\n",
    "train_data = TaggingDataset(ds.train, words, tags, True)\n",
    "dev_data = TaggingDataset(ds.validation, words, tags, True)\n",
    "test_data = TaggingDataset(ds.test, words, tags, False)\n",
    "\n",
    "\n",
    "logging.info(\"Vocabulary size: %d\", len(words))\n",
    "\n",
    "\n",
    "BATCH_SIZE=100\n",
    "\n",
    "train_loader = DataLoader(train_data, collate_fn=collate, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, collate_fn=collate, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, collate_fn=collate, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch): #Je défini une fonction pad_collate, qui garde les longeurs pour pouvoir recoller plus tard.\n",
    "    x_lens = [len(torch.LongTensor(b[0])) for b in batch]\n",
    "    x_pad = pad_sequence([torch.LongTensor(b[0]) for b in batch])\n",
    "    y_pad = pad_sequence([torch.LongTensor(b[1]) for b in batch])\n",
    "    \n",
    "    return(x_pad,y_pad,x_lens)\n",
    "    \n",
    "pckdtrain_loader = DataLoader(train_data,collate_fn = pad_collate,batch_size =BATCH_SIZE)\n",
    "pckdtest_loader =  DataLoader(test_data,collate_fn = pad_collate,batch_size =BATCH_SIZE)\n",
    "\n",
    "def eps_random(phrase,n_rd = 3): #Je prends maximum n_rd élements aléatoires par batch que je place hors du vocabulaire\n",
    "    for k in range(n_rd):\n",
    "        a,b = np.random.randint(phrase.size(0)),np.random.randint(phrase.size(1))\n",
    "        if phrase[a][b] != torch.tensor(0):\n",
    "            phrase[a][b] = torch.tensor(1)\n",
    "    return phrase\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):  \n",
    "    def __init__(self, dim_emb, dim_hidden, dim_out,n_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.emb = torch.nn.Embedding(len(words),dim_emb)\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(dim_emb,dim_hidden,n_layers)\n",
    "        self.decoder = nn.Linear(dim_hidden,dim_out)\n",
    "        \n",
    "    def forward(self, x, h, C):        \n",
    "        z = self.emb(x) \n",
    "        output,(h_n,c_n) = self.lstm(z,(h,C))\n",
    "        \n",
    "        return(output,h_n,c_n)\n",
    "    \n",
    "    def decode(self,h):\n",
    "        return(self.decoder(h))\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = Variable(next(self.parameters()).data.new(self.n_layers, batch_size, self.dim_hidden))\n",
    "        cell =  Variable(next(self.parameters()).data.new(self.n_layers, batch_size, self.dim_hidden))\n",
    "        return (hidden, cell)\n",
    "    \n",
    "class pckdLSTM(nn.Module):  ## Etant donné que je fais mon embedding dans le réseau, je dois défnir une classe adaptée.\n",
    "    def __init__(self, dim_emb, dim_hidden, dim_out,n_layers):\n",
    "        super(pckdLSTM, self).__init__()\n",
    "        \n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.emb = torch.nn.Embedding(len(words),dim_emb)\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(dim_emb,dim_hidden,n_layers)\n",
    "        self.decoder = nn.Linear(dim_hidden,dim_out)\n",
    "        \n",
    "    def forward(self, x, x_lens, h, C): #En effet,je dois \"pack\" ma sequence entre l'embedding et l'application du réseau      \n",
    "        x_emb = self.emb(x)\n",
    "        x_packed = pack_padded_sequence(x_emb,x_lens,enforce_sorted = False)\n",
    "        \n",
    "        output,(h_n,c_n) = self.lstm(x_packed,(h,C))\n",
    "        \n",
    "        return(output,h_n,c_n)\n",
    "    \n",
    "    def decode(self,h):\n",
    "        return(self.decoder(h))\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = Variable(next(self.parameters()).data.new(self.n_layers, batch_size, self.dim_hidden))\n",
    "        cell =  Variable(next(self.parameters()).data.new(self.n_layers, batch_size, self.dim_hidden))\n",
    "        return (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_epoch = 45\n",
    "batch_size = 100\n",
    "dim_hidden = 110\n",
    "dim_emb = 120\n",
    "dim_out = len(tags)\n",
    "n_layers = 5\n",
    "eps = 1e-3\n",
    "PAD = 0\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "network = LSTM(dim_emb, dim_hidden, dim_out,n_layers).to(device)\n",
    "optim = torch.optim.Adam(params = network.parameters(), lr = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacqu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\jacqu\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch loss is 2.4293168157964318\n",
      "epoch loss is 1.5138093654092375\n",
      "epoch loss is 0.8181695129487898\n",
      "epoch loss is 0.5795937486878642\n",
      "epoch loss is 0.44945026402706867\n",
      "epoch loss is 0.36055748383482017\n",
      "epoch loss is 0.29487476346792874\n",
      "epoch loss is 0.24846834131887743\n",
      "epoch loss is 0.2150629434760634\n",
      "epoch loss is 0.18896028170218834\n",
      "epoch loss is 0.16926902074080247\n",
      "epoch loss is 0.1522412282395196\n",
      "epoch loss is 0.1410315073245055\n",
      "epoch loss is 0.12834911746578617\n",
      "epoch loss is 0.11846521538454335\n",
      "epoch loss is 0.11805807814731464\n",
      "epoch loss is 0.10541823157272139\n",
      "epoch loss is 0.10214560234046483\n",
      "epoch loss is 0.09987329905266529\n",
      "epoch loss is 0.0883662862169159\n",
      "epoch loss is 0.0831603744698988\n",
      "epoch loss is 0.0840302440305273\n",
      "epoch loss is 0.07447263632308353\n",
      "epoch loss is 0.07245316458019344\n",
      "epoch loss is 0.06849516256080641\n",
      "epoch loss is 0.06750122119079936\n",
      "epoch loss is 0.07327670762901539\n",
      "epoch loss is 0.061644677593783066\n",
      "epoch loss is 0.057411526153971264\n",
      "epoch loss is 0.05719983335454147\n",
      "epoch loss is 0.05449898277858754\n",
      "epoch loss is 0.05900509707577579\n",
      "epoch loss is 0.05261949207503479\n",
      "epoch loss is 0.04841515097532656\n",
      "epoch loss is 0.05158601726008045\n",
      "epoch loss is 0.04656601008158047\n",
      "epoch loss is 0.04329185143529952\n",
      "epoch loss is 0.04344205282159619\n",
      "epoch loss is 0.04390189971227746\n",
      "epoch loss is 0.04688842236422575\n",
      "epoch loss is 0.04049084679438518\n",
      "epoch loss is 0.03839000008375078\n",
      "epoch loss is 0.03675910245653216\n",
      "epoch loss is 0.035715766485546016\n",
      "epoch loss is 0.03559341573423439\n",
      "Done 45 epoch in 4.4 minutes.\n"
     ]
    }
   ],
   "source": [
    "l = float('inf')\n",
    "t =  0\n",
    "list_loss_LSTM,epoch_loss_LSTM,Big_list_loss_LSTM = [],[],[]\n",
    "start = time.time()\n",
    "print(\"begin training\")\n",
    "for epoch in range(nbr_epoch):\n",
    "    \n",
    "    Big_list_loss_LSTM += list_loss_LSTM\n",
    "    epoch_loss_LSTM.append(np.mean(list_loss_LSTM))\n",
    "    list_loss_LSTM = []\n",
    "    \n",
    "    for batch_ndx, sample in enumerate(train_loader):\n",
    "\n",
    "        if batch_ndx == 143:\n",
    "            break        \n",
    "        phrases,target_tag = sample\n",
    "        phrases = phrases.to(device)\n",
    "        target_tag = target_tag.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        h_state,c_state = network.init_hidden(batch_size)\n",
    "        output,new_h,new_c = network.forward(phrases,h_state,c_state)\n",
    "        h = network.decode(output)\n",
    "        l = criterion(h.view((-1,len(tags))),target_tag.flatten())\n",
    "            \n",
    "        list_loss_LSTM.append(float(l))\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    print(\"epoch loss is\",np.mean(list_loss_LSTM) )    \n",
    "\n",
    "stop = time.time()\n",
    "print(f\"Done {nbr_epoch} epoch in {(stop-start)/60 :.2} minutes.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "dim_hidden = 110\n",
    "dim_emb = 120\n",
    "dim_out = len(tags)\n",
    "n_layers = 5\n",
    "eps = 1e-3\n",
    "PAD = 0\n",
    "criterion2 = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "network2 = pckdLSTM(dim_emb, dim_hidden, dim_out,n_layers).to(device)\n",
    "optim2 = torch.optim.Adam(params = network2.parameters(), lr = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "epoch loss is 2.426629521630027\n",
      "epoch loss is 1.6132978542701348\n",
      "epoch loss is 0.8952036987651478\n",
      "epoch loss is 0.5984157313416888\n",
      "epoch loss is 0.4545970529943079\n",
      "epoch loss is 0.36704570784435403\n",
      "epoch loss is 0.31047966096784685\n",
      "epoch loss is 0.26604184716731516\n",
      "epoch loss is 0.23051318682573893\n",
      "epoch loss is 0.20289656478208262\n",
      "epoch loss is 0.18149286446037827\n",
      "epoch loss is 0.16894725720573972\n",
      "epoch loss is 0.15767695629721754\n",
      "epoch loss is 0.14522215046665884\n",
      "epoch loss is 0.1331881036708405\n",
      "epoch loss is 0.12552198137228304\n",
      "epoch loss is 0.11755983552315852\n",
      "epoch loss is 0.11205398317400392\n",
      "epoch loss is 0.10926203299444039\n",
      "epoch loss is 0.10469266438817644\n",
      "epoch loss is 0.10033260978810438\n",
      "epoch loss is 0.10073527292563365\n",
      "epoch loss is 0.09718083777836153\n",
      "epoch loss is 0.09846188742797692\n",
      "epoch loss is 0.09553591320877308\n",
      "epoch loss is 0.08456916721550735\n",
      "epoch loss is 0.07649772869540261\n",
      "epoch loss is 0.07252417788743139\n",
      "epoch loss is 0.06970989391520306\n",
      "epoch loss is 0.06836752609147892\n",
      "epoch loss is 0.06779915805567395\n",
      "epoch loss is 0.06734882599928162\n",
      "epoch loss is 0.06900447721664722\n",
      "epoch loss is 0.0689660725997878\n",
      "epoch loss is 0.06995940943072726\n",
      "epoch loss is 0.06671971059241495\n",
      "epoch loss is 0.061200628200402625\n",
      "epoch loss is 0.05718824178814055\n",
      "epoch loss is 0.054194202477281746\n",
      "epoch loss is 0.052538128947461404\n",
      "epoch loss is 0.0531788488810921\n",
      "epoch loss is 0.05391895618628372\n",
      "epoch loss is 0.05283635587698513\n",
      "epoch loss is 0.05058056839018852\n",
      "epoch loss is 0.048780879279325055\n",
      "Done 45 epoch in 6.7 minutes.\n"
     ]
    }
   ],
   "source": [
    "l = float('inf')\n",
    "t =  0\n",
    "list_loss_LSTM2,epoch_loss_LSTM2,Big_list_loss_LSTM2 = [],[],[]\n",
    "start = time.time()\n",
    "print(\"begin training\")\n",
    "for epoch in range(nbr_epoch):\n",
    "    \n",
    "    Big_list_loss_LSTM2 += list_loss_LSTM2\n",
    "    epoch_loss_LSTM2.append(np.mean(list_loss_LSTM2))\n",
    "    list_loss_LSTM2 = []\n",
    "    \n",
    "    for batch_ndx, sample in enumerate(pckdtrain_loader):\n",
    "\n",
    "        if batch_ndx == 143:\n",
    "            break        \n",
    "        phrases,target_tag,phrase_len = sample\n",
    "        phrases = phrases.to(device)\n",
    "        target_tag = target_tag.to(device)\n",
    "        #phrase_len = phrase_len.to(device)\n",
    "        \n",
    "        optim2.zero_grad()\n",
    "        \n",
    "        h_state,c_state = network2.init_hidden(batch_size)\n",
    "        output_packed,new_h,new_c = network2.forward(phrases,phrase_len,h_state,c_state)\n",
    "        output,output_lengths = pad_packed_sequence(output_packed)\n",
    "        h = network2.decode(output)\n",
    "        l = criterion2(h.view((-1,len(tags))),target_tag.flatten())\n",
    "            \n",
    "        list_loss_LSTM2.append(float(l))\n",
    "        l.backward()\n",
    "        optim2.step()\n",
    "        \n",
    "    print(\"epoch loss is\",np.mean(list_loss_LSTM2) )    \n",
    "    \n",
    "\n",
    "stop = time.time()\n",
    "print(f\"Done {nbr_epoch} epoch in {(stop-start)/60 :.2} minutes.\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "dim_hidden = 110\n",
    "dim_emb = 120\n",
    "dim_out = len(tags)\n",
    "n_layers = 6\n",
    "eps = 5e-4\n",
    "PAD = 0\n",
    "criterion3 = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "network3 = pckdLSTM(dim_emb, dim_hidden, dim_out,n_layers).to(device)\n",
    "optim3 = torch.optim.Adam(params = network3.parameters(), lr = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "epoch loss is 2.4707230587939284\n",
      "epoch loss is 2.36259749719313\n",
      "epoch loss is 2.1373294566894745\n",
      "epoch loss is 1.2893585654405446\n",
      "epoch loss is 0.8783888896028479\n",
      "epoch loss is 0.6841358196485293\n",
      "epoch loss is 0.5616360202535883\n",
      "epoch loss is 0.4794025685820546\n",
      "epoch loss is 0.416143317322631\n",
      "epoch loss is 0.3699703018565278\n",
      "epoch loss is 0.325891516216985\n",
      "epoch loss is 0.29300022750467686\n",
      "epoch loss is 0.27052881167485165\n",
      "epoch loss is 0.25297793047828276\n",
      "epoch loss is 0.2297815939971617\n",
      "epoch loss is 0.21170861383418102\n",
      "epoch loss is 0.19927892078469683\n",
      "epoch loss is 0.18517055221787698\n",
      "epoch loss is 0.17356570533939175\n",
      "epoch loss is 0.16553610672692318\n",
      "epoch loss is 0.1596308444867601\n",
      "epoch loss is 0.1538809618124595\n",
      "epoch loss is 0.15275356335031404\n",
      "epoch loss is 0.1511603666769041\n",
      "epoch loss is 0.14159567351941463\n",
      "epoch loss is 0.13522665150515684\n",
      "epoch loss is 0.13304988049960637\n",
      "epoch loss is 0.1343920980091695\n",
      "epoch loss is 0.13910254711037748\n",
      "epoch loss is 0.13997628388704952\n",
      "epoch loss is 0.11423019804320969\n",
      "epoch loss is 0.10634082805860293\n",
      "epoch loss is 0.10124321849821331\n",
      "epoch loss is 0.09885117349091109\n",
      "epoch loss is 0.09679175387104075\n",
      "epoch loss is 0.09424132509873463\n",
      "epoch loss is 0.09231151635204995\n",
      "epoch loss is 0.09106267321776677\n",
      "epoch loss is 0.09050831584246842\n",
      "epoch loss is 0.0923803600293773\n",
      "epoch loss is 0.09598690214065406\n",
      "epoch loss is 0.10115923867984251\n",
      "epoch loss is 0.09443848238854141\n",
      "epoch loss is 0.08848926187916235\n",
      "epoch loss is 0.08533558999741828\n",
      "Done 45 epoch in 7.8 minutes.\n"
     ]
    }
   ],
   "source": [
    "l = float('inf')\n",
    "t =  0\n",
    "list_loss_LSTM3,epoch_loss_LSTM3,Big_list_loss_LSTM3 = [],[],[]\n",
    "start = time.time()\n",
    "print(\"begin training\")\n",
    "for epoch in range(nbr_epoch):\n",
    "    \n",
    "    Big_list_loss_LSTM3 += list_loss_LSTM3\n",
    "    epoch_loss_LSTM3.append(np.mean(list_loss_LSTM3))\n",
    "    list_loss_LSTM3 = []\n",
    "    \n",
    "    for batch_ndx, sample in enumerate(pckdtrain_loader):\n",
    "\n",
    "        if batch_ndx == 143:\n",
    "            break        \n",
    "        phrases,target_tag,phrase_len = sample\n",
    "        phrases_rd = eps_random(phrases)  \"\"\"\"Je randomise un ou deux élément des phrases\"\"\"\n",
    "        phrases = phrases_rd.to(device)\n",
    "        target_tag = target_tag.to(device)\n",
    "        #phrase_len = phrase_len.to(device)\n",
    "        \n",
    "        optim3.zero_grad()\n",
    "        \n",
    "        h_state,c_state = network3.init_hidden(batch_size)\n",
    "        output_packed,new_h,new_c = network3.forward(phrases,phrase_len,h_state,c_state)\n",
    "        output,output_lengths = pad_packed_sequence(output_packed)\n",
    "        h = network3.decode(output)\n",
    "        l = criterion3(h.view((-1,len(tags))),target_tag.flatten())\n",
    "            \n",
    "        list_loss_LSTM3.append(float(l))\n",
    "        l.backward()\n",
    "        optim3.step()\n",
    "        \n",
    "    print(\"epoch loss is\",np.mean(list_loss_LSTM3))    \n",
    "\n",
    "stop = time.time()\n",
    "print(f\"Done {nbr_epoch} epoch in {(stop-start)/60 :.2} minutes.\")    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(epoch_loss_LSTM3)),epoch_loss_LSTM3,label = 'loss LSTM randomized')\n",
    "ax.plot(np.arange(len(epoch_loss_LSTM2)),epoch_loss_LSTM2,label = 'loss LSTM packed')\n",
    "ax.plot(np.arange(len(epoch_loss_LSTM)),epoch_loss_LSTM,label = 'loss LSTM')\n",
    "ax.set_title(\"Loss amelioration by epoch\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"SoftMaxLoss\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss is 0.7632154776499822\n",
      "pckd test loss is 0.9133736261954675\n",
      "randomized test loss is 1.1438544300886302\n",
      "prévisions: tensor([11, 12, 16,  1,  2,  7,  3, 10,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7]) torch.Size([86])\n",
      "vs\n",
      "réalité: tensor([ 1,  2,  5,  1,  2,  7,  2, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       device='cuda:0') torch.Size([86])\n",
      "le padding ignore les 0 au bout de séquence !\n"
     ]
    }
   ],
   "source": [
    "##Un peu de Test:\n",
    "test_loss = []\n",
    "for batch_ndx, sample in enumerate(test_loader):\n",
    "\n",
    "        if batch_ndx == 13:\n",
    "            break        \n",
    "        phrases,target_tag = sample\n",
    "        phrases = phrases.to(device)\n",
    "        target_tag = target_tag.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        h_state,c_state = network.init_hidden(batch_size)\n",
    "        output,new_h,new_c = network.forward(phrases,h_state,c_state)\n",
    "        h = network.decode(output)\n",
    "        l = criterion(h.view((-1,len(tags))),target_tag.flatten())\n",
    "            \n",
    "        test_loss.append(float(l))\n",
    "          \n",
    "print(\"test loss is\",np.mean(test_loss))\n",
    "pckdtest_loss = []\n",
    "for batch_ndx, sample in enumerate(pckdtest_loader):\n",
    "\n",
    "        if batch_ndx == 13:\n",
    "            break        \n",
    "        phrases,target_tag,phrase_len = sample\n",
    "        phrases = phrases.to(device)\n",
    "        target_tag = target_tag.to(device)\n",
    "        #phrase_len = phrase_len.to(device)\n",
    "        \n",
    "        optim2.zero_grad()\n",
    "        \n",
    "        h_state,c_state = network2.init_hidden(batch_size)\n",
    "        output_packed,new_h,new_c = network2.forward(phrases,phrase_len,h_state,c_state)\n",
    "        output,output_lengths = pad_packed_sequence(output_packed)\n",
    "        h = network2.decode(output)\n",
    "        l = criterion2(h.view((-1,len(tags))),target_tag.flatten())\n",
    "            \n",
    "        pckdtest_loss.append(float(l))\n",
    "                \n",
    "print(\"pckd test loss is\",np.mean(pckdtest_loss))\n",
    "\n",
    "rd_test_loss = []\n",
    "for batch_ndx, sample in enumerate(pckdtest_loader):\n",
    "\n",
    "        if batch_ndx == 13:\n",
    "            break        \n",
    "        phrases,target_tag,phrase_len = sample\n",
    "        phrases = phrases.to(device)\n",
    "        target_tag = target_tag.to(device)\n",
    "        #phrase_len = phrase_len.to(device)\n",
    "        \n",
    "        optim3.zero_grad()\n",
    "        \n",
    "        h_state,c_state = network3.init_hidden(batch_size)\n",
    "        output_packed,new_h,new_c = network3.forward(phrases,phrase_len,h_state,c_state)\n",
    "        output,output_lengths = pad_packed_sequence(output_packed)\n",
    "        h = network3.decode(output)\n",
    "        l = criterion3(h.view((-1,len(tags))),target_tag.flatten())\n",
    "            \n",
    "        rd_test_loss.append(float(l))\n",
    "        \n",
    "        \n",
    "print(\"randomized test loss is\",np.mean(rd_test_loss))\n",
    "\n",
    "\n",
    "\n",
    "L = torch.tensor([torch.argmax(k) for k in h[:,0]])\n",
    "print('prévisions:',L,L.shape)\n",
    "print('vs')\n",
    "print('réalité:',target_tag[:,0],target_tag[:,0].shape)\n",
    "print('le padding ignore les 0 au bout de séquence !')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilan:\n",
    "les courbes d'apprentissage ont exactement la tête attendue, mais le \"randomized\" a les moins bons résultats en test, alors qu'il devrait avoir les meilleurs, car il sait a priori gérer des mots inconnus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
